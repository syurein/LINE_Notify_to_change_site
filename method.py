import asyncio
from playwright.async_api import async_playwright
import time
async def scrape_web_data(url: str, mode: str):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ã€ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¾ã™ã€‚
    - 'aria-label': å…¨ã¦ã®aria-labelå±æ€§ã‚’å–å¾—
    - 'a-title': å…¨ã¦ã®<a>ã‚¿ã‚°ã®titleå±æ€§ã‚’å–å¾—
    - 'span-content': ç‰¹å®šã®classã‚’æŒã¤<span>ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    """
    
    # æœ‰åŠ¹ãªãƒ¢ãƒ¼ãƒ‰ã‚’å®šç¾©
    valid_modes = ['aria-label', 'a-title', 'span-content']
    if mode not in valid_modes:
        print(f"ğŸš¨ã‚¨ãƒ©ãƒ¼: ç„¡åŠ¹ãªãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚'{', '.join(valid_modes)}'ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        return

    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()

        try:
            print(f"'{url}' ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã„ã¾ã™...")
            # ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒè½ã¡ç€ãã¾ã§å¾…æ©Ÿ
            await page.goto(url)
            time.sleep(10)
            print(f"ãƒ¢ãƒ¼ãƒ‰ '{mode}' ã§ãƒ‡ãƒ¼ã‚¿ã®åé›†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

            data_list = []
            
            # --- ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å² ---
            
            if mode == 'aria-label':#ãƒ¡ãƒ«ã‚«ãƒªã€ã‚¢ãƒã‚¾ãƒ³ç”¨
                # [aria-label]å±æ€§ã‚’æŒã¤ã™ã¹ã¦ã®è¦ç´ ã‹ã‚‰ãã®å€¤ã‚’å–å¾—
                locator = page.locator('div[aria-label]')
                data_list = await locator.evaluate_all(
                    "(elements) => elements.map(el => el.getAttribute('aria-label'))"
                )
                
            elif mode == 'a-title':#æ¥½å¤©ç”¨
                # titleå±æ€§ã‚’æŒã¤å…¨ã¦ã®<a>ã‚¿ã‚°ã‹ã‚‰ãã®å€¤ã‚’å–å¾—
                locator = page.locator('a[title]')
                data_list = await locator.evaluate_all(
                    "(elements) => elements.map(el => el.getAttribute('title'))"
                )

            elif mode == 'span-content':#yahooç”¨
                # classãŒ'SearchResultItemTitle'ã§ã‚ã‚‹<span>ã‚¿ã‚°ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’å…¨ã¦å–å¾—
                # ã“ã®å ´åˆã€all_text_contents()ãŒä¾¿åˆ©ã§ã™
                locator = page.locator('span[class*="SearchResultItemTitle"]')
                data_list = await locator.all_text_contents()

            # --- åé›†çµæœã®å‡ºåŠ› ---

            # ç©ºã®è¦ç´ ã‚„Noneã‚’å–ã‚Šé™¤ã
            filtered_data = [item.strip() for item in data_list if item and item.strip()]

            if filtered_data:
                print(f"\nâœ… {len(filtered_data)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ:")
                for i, item in enumerate(filtered_data, 1):
                    print(f"  {i}: {item}")
            else:
                print("\nâŒ å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        except Exception as e:
            print(f"\nğŸš¨ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        finally:
            await browser.close()
            print("\nãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã—ãŸã€‚")

# --- è¨­å®šã“ã“ã‹ã‚‰ ---

# 1. èª¿æŸ»ã—ãŸã„ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®URLã‚’æŒ‡å®š
target_url = "" 

# 2. å®Ÿè¡Œã—ãŸã„ãƒ¢ãƒ¼ãƒ‰ã‚’ 'aria-label', 'a-title', 'span-content' ã‹ã‚‰é¸ã‚“ã§æŒ‡å®š
scrape_mode = 'span-content'  # â† ã“ã“ã‚’æ›¸ãæ›ãˆã¦ãã ã•ã„

# --- è¨­å®šã“ã“ã¾ã§ ---


# éåŒæœŸé–¢æ•°ã‚’å®Ÿè¡Œ
if __name__ == "__main__":
    asyncio.run(scrape_web_data(target_url, scrape_mode))